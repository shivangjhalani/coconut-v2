{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395e113-10ee-43e9-8182-6b0a2cbd1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements-my.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97890d6c-d989-425f-84c9-da03bf663464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgunadeep2005\u001b[0m (\u001b[33mgunadeep2005-pes-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8cd39f-e68d-45d5-a743-58e6b5fcb9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'project': 'coconut', 'save_path': 'checkpoints', 'name': 'prosqa-coconut', 'only_eval': False, 'coconut': True, 'cot': False, 'no_thoughts': False, 'no_cot': False, 'c_thought': 1, 'epochs_per_stage': 5, 'max_latent_stage': 6, 'pad_latent_to_max': True, 'save_only_improve': False, 'uniform_prob': 0.0, 'model_id': 'openai-community/gpt2', 'load_model_path': 'None', 'seed': 0, 'resume': 0, 'bf16': False, 'train_path': 'data/prosqa_train.json', 'val_path': 'data/prosqa_valid.json', 'reset_optimizer': True, 'batch_size_training': 8, 'debug': False, 'gradient_accumulation_steps': 1, 'num_epochs': 50, 'lr': 0.0001, 'weight_decay': 0.01}\n",
      "[rank0]:[W621 17:21:12.841860301 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "Running FSDP on rank = 0, world size = 1\n",
      "/opt/jupyter_venv/lib/python3.10/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.\n",
      "  warnings.warn(\n",
      "FullyShardedDataParallel(\n",
      "  (_fsdp_wrapped_module): Coconut(\n",
      "    (base_causallm): GPT2LMHeadModel(\n",
      "      (transformer): GPT2Model(\n",
      "        (wte): Embedding(50260, 768)\n",
      "        (wpe): Embedding(1024, 768)\n",
      "        (drop): Dropout(p=0.1, inplace=False)\n",
      "        (h): ModuleList(\n",
      "          (0-11): 12 x GPT2Block(\n",
      "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): GPT2SdpaAttention(\n",
      "              (c_attn): Conv1D(nf=2304, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=768)\n",
      "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): GPT2MLP(\n",
      "              (c_fc): Conv1D(nf=3072, nx=768)\n",
      "              (c_proj): Conv1D(nf=768, nx=3072)\n",
      "              (act): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
      "    )\n",
      "    (embedding): Embedding(50260, 768)\n",
      "  )\n",
      ")\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:01<00:00, 206.78 examples/s]\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17886/17886 [00:06<00:00, 2742.17 examples/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgunadeep2005\u001b[0m (\u001b[33mgunadeep2005-pes-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/coconut-v2/wandb/run-20250621_172127-xjwwllww\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mprosqa-coconut\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/gunadeep2005-pes-university/coconut\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/gunadeep2005-pes-university/coconut/runs/xjwwllww\u001b[0m\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 672.38 examples/s]\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17886/17886 [00:01<00:00, 11700.95 examples/s]\n",
      "Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 603.21 examples/s]\n",
      "Training Epoch: 1:   0%|\u001b[34m                               \u001b[0m| 0/2236 [00:00<?, ?it/s]\u001b[0mlogging training data\n",
      "Training Epoch: 1/50, batch 669/2236 completed (loss: 0.0662:  30%|\u001b[34mâ–Ž\u001b[0m| 670/2236 [\u001b[0m"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "!torchrun --nnodes 1 --nproc_per_node 1 run.py args/prosqa_coconut.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4551b5bb-caf4-4424-9b9c-a40949c0e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the checkpoint with best validation accuracy, and put the path as `load_model_path` in [args/prosqa_coconut_eval.yaml](args/prosqa_coconut_eval.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70442a3f-4484-4c4a-9325-a3f897ea32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL\n",
    "!torchrun --nnodes 1 --nproc_per_node 4 run.py args/prosqa_coconut_eval.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
